{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIgM6C9HYUhm"
   },
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n",
    "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
    "\n",
    "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
    "\n",
    "Useful links:\n",
    "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
    "- [Norvig's dataset](https://norvig.com/big.txt)\n",
    "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
    "\n",
    "Grading:\n",
    "- 60 points - Implement spelling correction\n",
    "- 20 points - Justify your decisions\n",
    "- 20 points - Evaluate on a test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-vb8yFOGRDF"
   },
   "source": [
    "## Implement context-sensitive spelling correction\n",
    "\n",
    "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
    "\n",
    "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
    "\n",
    "You may also want to implement:\n",
    "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
    "- some recent (or not very recent) paper on this topic,\n",
    "- solution which takes into account keyboard layout and associated misspellings,\n",
    "- efficiency improvement to make the solution faster,\n",
    "- any other idea of yours to improve the Norvig’s solution.\n",
    "\n",
    "IMPORTANT:  \n",
    "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
    "- Your implementation\n",
    "- Analysis of why the implemented approach is suggested\n",
    "- Improvements of the original approach that you have chosen to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for context-sensitive spelling corrector using N-gram language models\n",
    "import re\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def load_ngrams(file_path):\n",
    "    \"\"\"Read N-grams from dataset file\"\"\"\n",
    "    n_grams = defaultdict(float) # Use defaultdict to handle missing keys\n",
    "    total_n_grams = 0\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            count, ngram = float(re.findall(r\"\\w+\", line)[0]), '|'.join([l for l in re.findall(r\"\\w+\", line)[1:]]) # Split line into count and n-gram\n",
    "            n_grams[ngram] += count # Update the count for the n-gram\n",
    "            total_n_grams += count\n",
    "        \n",
    "    for ngram in n_grams:\n",
    "        n_grams[ngram] /= total_n_grams\n",
    "    return n_grams\n",
    "\n",
    "# Load bigram and 5-gram data\n",
    "bigram_data = load_ngrams(\"data/w2_.txt\")\n",
    "fivegram_data = load_ngrams(\"data/w5_.txt\")\n",
    "\n",
    "# Merge bigram and 5-gram data\n",
    "combined_ngrams = defaultdict(float)\n",
    "combined_ngrams.update(bigram_data)\n",
    "combined_ngrams.update(fivegram_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total N-grams: 2128532\n",
      "Total words: 64296\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total N-grams: {len(combined_ngrams)}\")\n",
    "# Count the words in combined_ngrams\n",
    "words = Counter('|'.join(combined_ngrams).split('|'))\n",
    "print(f\"Total words: {len(words)}\")\n",
    "\n",
    "# Normalize frequencies by total word count\n",
    "total_count = sum(words.values())\n",
    "for word in words:\n",
    "    combined_ngrams[word] = words[word] / total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Define the keyboard layout\n",
    "keyboard = {\n",
    "    'q': (0, 0), 'w': (0, 1), 'e': (0, 2), 'r': (0, 3), 't': (0, 4), 'y': (0, 5), 'u': (0, 6), 'i': (0, 7), 'o': (0, 8), 'p': (0, 9),\n",
    "    'a': (1, 0), 's': (1, 1), 'd': (1, 2), 'f': (1, 3), 'g': (1, 4), 'h': (1, 5), 'j': (1, 6), 'k': (1, 7), 'l': (1, 8),\n",
    "    'z': (2, 0), 'x': (2, 1), 'c': (2, 2), 'v': (2, 3), 'b': (2, 4), 'n': (2, 5), 'm': (2, 6)\n",
    "}\n",
    "\n",
    "def calculate_distance(key1, key2):\n",
    "    \"\"\"Calculate the Euclidean distance between two keys on the keyboard.\"\"\"\n",
    "    if key1 == '|':\n",
    "        x1, y1 = -1, -1\n",
    "    else:\n",
    "        x1, y1 = keyboard[key1]\n",
    "    if key2 == '|':\n",
    "        x2, y2 = -1, -1\n",
    "    else:\n",
    "        x2, y2 = keyboard[key2]\n",
    "    return np.abs(x1 - x2) + np.abs(y1 - y2)\n",
    "\n",
    "# Test the function\n",
    "assert calculate_distance(\"z\", \"z\") == 0\n",
    "assert calculate_distance(\"q\", \"p\") == len(\"qwertyuiop\") - 1\n",
    "assert calculate_distance(\"q\", \"m\") == 8\n",
    "print(calculate_distance('o', 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit distance algorithm Based on the Damerau-Levenshtein distance (https://www.guyrutenberg.com/2008/12/15/damerau-levenshtein-distance-in-python/)\n",
    "def edit_distance(s1, s2, is_cost=False):\n",
    "    \"\"\"Calculate the Damerau-Levenshtein distance between two strings.\"\"\"\n",
    "    if len(s1) > len(s2):\n",
    "        return edit_distance(s2, s1, is_cost)\n",
    "\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    def del_cost(a, b):\n",
    "        if not is_cost:\n",
    "            return 1\n",
    "        else:\n",
    "            return calculate_distance(a, b) if a!=b else 0.5\n",
    "\n",
    "    def ins_cost(a, b):\n",
    "        if not is_cost:\n",
    "            return 1\n",
    "        else:\n",
    "            return calculate_distance(a, b) if a!=b else 0.5\n",
    "\n",
    "    def sub_cost(a, b):\n",
    "        return calculate_distance(a, b) if is_cost else 1\n",
    "\n",
    "    d = np.zeros((len(s1)+1, len(s2)+1))\n",
    "    for i in range(len(s1)+1):\n",
    "        d[i][0] = i\n",
    "    for j in range(len(s2)+1):\n",
    "        d[0][j] = j\n",
    "    \n",
    "\n",
    "    for i in range(1, len(s1)+1):\n",
    "        for j in range(1, len(s2)+1):\n",
    "            c1 = s1[i-1]\n",
    "            c2 = s2[j-1]\n",
    "\n",
    "            insertions = d[i][j-1] + ins_cost(c1, c2)\n",
    "            deletions = d[i-1][j] + del_cost(c1, c2)\n",
    "            substitutions = d[i-1][j-1] + sub_cost(c1, c2)*(1 if c1!=c2 else 0)\n",
    "            d[i][j] = min(insertions, deletions, substitutions)\n",
    "\n",
    "            x1 = s1[i-2]\n",
    "            x2 = s2[j-2]\n",
    "\n",
    "            if i>2 and j>2 and c1 == x2 and c2 == x1:\n",
    "                d[i, j] = np.min([d[i, j], d[i - 3][j - 3] + 1])\n",
    "\n",
    "    return d[-1][-1]\n",
    "\n",
    "\n",
    "# Test the function\n",
    "assert edit_distance(\"alright\", \"alrigkt\") == 1\n",
    "assert edit_distance('', 'Kazan') == 5\n",
    "assert edit_distance('Foreign', 'foregin') == 2\n",
    "assert edit_distance('kingsdom', 'knigsdoom', True) == 1.5\n",
    "assert edit_distance('love', 'lave', True) == 5.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word(w, n, words):\n",
    "    for word in words:\n",
    "        state = (range(n+1), range(n+1))\n",
    "        for c in word: \n",
    "            idx, values = state\n",
    "            if idx and idx[0] == 0 and values[0] < n:\n",
    "                new_idx = [0]\n",
    "                new_values = [values[0] + 1]\n",
    "            else:\n",
    "                new_idx = []\n",
    "                new_values = []\n",
    "\n",
    "            for j,i in enumerate(idx):\n",
    "                if i == len(w): break\n",
    "                cost = 0 if w[i] == c else 1\n",
    "                val = values[j] + cost\n",
    "                if new_idx and new_idx[-1] == i:\n",
    "                    val = min(val, new_values[-1] + 1)\n",
    "                if j+1 < len(idx) and idx[j+1] == i+1:\n",
    "                    val = min(val, values[j+1] + 1)\n",
    "                if val <= n:\n",
    "                    new_idx.append(i+1)\n",
    "                    new_values.append(val)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            state = (new_idx, new_values)\n",
    "        if bool(state[0]) and state[0][-1] == len(w): \n",
    "            yield word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_edits_using_n_gram(n_gram, n=2, edits=2):\n",
    "    n_grams_ = ['|'.join(n_gram[i:i+n]) for i in range(len(n_gram) - n + 1)]\n",
    "    g_edits = [defaultdict(lambda: 0) for _ in n_gram]\n",
    "\n",
    "    for i, x in enumerate(n_grams_):\n",
    "        edits_ = list(map(lambda y: y.split('|'), find_word(x, edits, combined_ngrams)))\n",
    "        for edit in edits_:\n",
    "            for j, token in enumerate(edit):\n",
    "                g_edits[i + j][token] += combined_ngrams['|'.join(edit)]\n",
    "    res = [dict(x) for x in g_edits]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'i': 1.7019393599006068e-06},\n",
       " {'am': 1.7019393599006068e-06},\n",
       " {'going': 1.7019393599006068e-06},\n",
       " {'to': 1.7019393599006068e-06},\n",
       " {'the': 1.7019393599006068e-06},\n",
       " {}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_edits_using_n_gram([\"i\", \"am\", \"giong\", \"to\", \"the\", \"park\"], n=5, edits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': 2.409695644420373e-06,\n",
       "  'i': 0.0004278343127868501,\n",
       "  'in': 1.0496648176129264e-06,\n",
       "  'is': 1.1996169344147731e-06,\n",
       "  'it': 6.800154134037232e-07,\n",
       "  'j': 1.2902856562019363e-07},\n",
       " {'cook': 4.191684753391155e-06,\n",
       "  'cooked': 2.2283582008460467e-06,\n",
       "  'booked': 2.092355118165302e-07,\n",
       "  'choke': 1.2205404855964261e-07,\n",
       "  'choked': 3.347768189064483e-07,\n",
       "  'cocked': 8.718146325688759e-08,\n",
       "  'could': 0.0003057942132613286,\n",
       "  'good': 1.1507953149909161e-07,\n",
       "  'hook': 9.76432388477141e-08,\n",
       "  'hooked': 4.98677969829397e-07,\n",
       "  'look': 3.421349344053296e-05,\n",
       "  'looked': 4.231788226489323e-05,\n",
       "  'took': 4.2663120859390504e-05,\n",
       "  'cooks': 4.289327992238869e-07},\n",
       " {'fortune': 5.47499589253254e-07},\n",
       " {'cookie': 3.243150433156218e-07, 'cookies': 2.2318454593763221e-07}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_edits_using_n_gram([\"i\", \"cookd\", \"fortuna\", \"cookis\"], n=2, edits=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sentence(sentence, n=2, edits=2):\n",
    "    # Разбиваем предложение на слова\n",
    "    words = re.findall(r\"\\w+\", sentence.lower())\n",
    "    \n",
    "    # Получаем возможные исправления и их оценки\n",
    "    word = \"|\".join(words)\n",
    "    corrections = np.array(list_edits_using_n_gram(words, n, edits), dtype=object)\n",
    "    \n",
    "    # Применяем исправления к словам\n",
    "    corrected_words = []\n",
    "    for i, word in enumerate(words):\n",
    "        # Выбираем наиболее подходящее исправление для текущего слова\n",
    "        best_correction = None\n",
    "        best_score = -1\n",
    "        for correction, score in corrections[i].items():\n",
    "            if score > best_score:\n",
    "                best_correction = correction\n",
    "                best_score = score\n",
    "        \n",
    "        # Если для слова найдено исправление, применяем его\n",
    "        if best_correction:\n",
    "            corrected_words.append(best_correction)\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    \n",
    "    # Возвращаем исправленное предложение\n",
    "    return ' '.join(corrected_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oML-5sJwGRLE"
   },
   "source": [
    "## Justify your decisions\n",
    "\n",
    "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
    "- Which ngram dataset to use\n",
    "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
    "- Beam search parameters\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Xb_twOmVsC6"
   },
   "source": [
    "The way I try to fix mistakes in grammar is by looking at groups of words. I look at groups of 5 words, 2 words, and just one word. I found that using just one size of group was not enough. If I use too small of a group, the code might not understand the whole meaning. But if I use too big of a group, the code might not find enough examples to help fix the mistake. I made a system that helps choose the best word to replace another word. It looks at how often different words can be used and also how similar they are to the original word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46rk65S4GRSe"
   },
   "source": [
    "## Evaluate on a test set\n",
    "\n",
    "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that compares two texts and returns \n",
    "# the number of matches and differences\n",
    "from itertools import zip_longest\n",
    "\n",
    "def compare(text1, text2):\n",
    "    l1 = text1.lower().split()\n",
    "    l2 = text2.lower().split()\n",
    "    good = 0\n",
    "    bad = 0\n",
    "    for word1, word2 in zip_longest(l1, l2, fillvalue=''):\n",
    "        if word1 != word2:\n",
    "            bad += 1\n",
    "        else:\n",
    "            good += 1\n",
    "    return (good, bad)\n",
    "\n",
    "# Helper function to calculate the percentage of misspelled words\n",
    "def percentageOfBad(x):\n",
    "    return (x[1] / (x[0] + x[1])) * 100\n",
    "\n",
    "def test_func(corrects, incorrects):\n",
    "    err = 0\n",
    "    res = []\n",
    "    for corct, incorrect in zip(corrects, incorrects):\n",
    "        if len(incorrect) > 5:\n",
    "            corrected = correct_sentence(incorrect, n=5, edits=3)\n",
    "            # corrected = correct_sentence(corrected, n=2, edits=2)\n",
    "        else:\n",
    "            corrected = correct_sentence(incorrect, n=2, edits=2)\n",
    "        # corrected = correct_sentence(corrected, n=1, edits=2)\n",
    "\n",
    "        err = compare(corrected, corct)\n",
    "        perc = percentageOfBad(err)\n",
    "        res.append((perc, corrected))\n",
    "    return res\n",
    "\n",
    "# print(test_func([\"I am going to the park\"], [\"I am giong to the park\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of correct sentences and its incorrect versions\n",
    "corrects = [\"I am going to the park\", \"I don't like to play football\", \"I love to cook and bake\", \"He is very tall\", \"She is very beautiful\",\n",
    "            \"We are going to the cinema tonight\", \"The sun is shining brightly\", \"He is my best friend\", \"I don't know what to do\", \"I have a cat\"\n",
    "            \"We are eating pizza\", \"She is reading a book\", \"The quick brown fox jumps over the lazy dog\", \"I am a student\", \"I love to eat apples\"]\n",
    "incorrects = [\"I am giong to th park\", \"I don't lke to paly football\", \"I lpve to cok and bske\", \"Hhe is veri tal\", \"Sh ise vary butiful\",\n",
    "            \"We are ging to the cnema tonit\", \"The sun is shinng brighty\", \"He is my bst freind\", \"I dont kbow wgat to do\", \"I hve a ct\"\n",
    "            \"We are eat pizza\", \"She is reading a bok\", \"The qwick brow fox jump over the lazy dog\", \"I m a stdent\", \"I lovee too eat aples\"]\n",
    "\n",
    "ans_my = test_func(corrects, incorrects)\n",
    "print(ans_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.42743764172336\n"
     ]
    }
   ],
   "source": [
    "print(sum(ans_my[i][0] for i in range(len(ans_my))) / len(ans_my))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norvig Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "OwZWaX9VVs7B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(16.666666666666664, ['i', 'am', 'going', 'to', 'th', 'park']), (28.57142857142857, ['i', 'don', 't', 'le', 'to', 'pay', 'football']), (50.0, ['i', 'le', 'to', 'co', 'and', 'bse']), (50.0, ['he', 'is', 'ver', 'tal']), (100.0, ['sh', 'ise', 'vary', 'butiful']), (42.857142857142854, ['we', 'are', 'ing', 'to', 'the', 'nea', 'toni']), (40.0, ['the', 'sun', 'is', 'shinn', 'bright']), (20.0, ['he', 'is', 'my', 'st', 'friend']), (85.71428571428571, ['i', 'dont', 'bow', 'wat', 'to', 'do']), (42.857142857142854, ['i', 'he', 'a', 'cte', 'are', 'eat', 'pizza']), (20.0, ['she', 'is', 'reading', 'a', 'bok']), (33.33333333333333, ['the', 'wick', 'brow', 'fox', 'jump', 'over', 'the', 'lazy', 'dog']), (50.0, ['i', 'm', 'a', 'stent']), (40.0, ['i', 'love', 'too', 'eat', 'apes'])]\n",
      "44.285714285714285\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def P(word, N=sum(words.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return words[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words_): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words_ if w in words)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'абвгдеёжзийклмнопрстуфхцчшщьыъэюя'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "def norvig(corrects, incorrects):\n",
    "    corrects = [re.findall(\"\\w+\", i.lower()) for i in corrects]\n",
    "    incorrects = [re.findall(\"\\w+\", i.lower()) for i in incorrects]\n",
    "    err = 0\n",
    "    res = []\n",
    "    for corct, incorrect in zip(corrects, incorrects):\n",
    "        corrected = []\n",
    "        for i in range(len(incorrect)):\n",
    "            corrected.append(correction(incorrect[i]) )\n",
    "        \n",
    "        err = compare(\" \".join(corrected), \" \".join(corct))\n",
    "        perc = percentageOfBad(err)\n",
    "        res.append((perc, corrected))\n",
    "    return res\n",
    "\n",
    "ans_norvig = norvig(corrects, incorrects)\n",
    "print(ans_norvig)\n",
    "print(sum(ans_norvig[i][0] for i in range(len(ans_norvig))) / len(ans_norvig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
