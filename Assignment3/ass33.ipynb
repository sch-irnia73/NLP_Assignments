{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for file path handling, PyTorch Lightning, PyTorch, JSON, OS, IOBES, Pandas, and regular expressions\n",
    "from pathlib import Path\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import iobes\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Import tqdm for progress bars, DataLoader and Dataset from PyTorch for data handling, and Trainer from PyTorch Lightning for training\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "# Import NLTK for natural language processing tasks\n",
    "from nltk.data import load\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# Import transformers for pre-trained models and schedules, and logging for logging purposes\n",
    "from transformers import BertForTokenClassification, AdamW, get_linear_schedule_with_warmup, logging\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ners</th>\n",
       "      <th>sentences</th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>spans</th>\n",
       "      <th>span_ners</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0, 5, CITY], [16, 23, PERSON], [34, 41, PERS...</td>\n",
       "      <td>Бостон взорвали Тамерлан и Джохар Царнаевы из ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[(Бостон, 0, 5), (взорвали, 7, 14), (Тамерлан,...</td>\n",
       "      <td>[(0, 5), (7, 14), (16, 23), (25, 25), (27, 32)...</td>\n",
       "      <td>[((0, 0), (0, 5)), ((2, 2), (16, 23)), ((5, 5)...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[21, 28, PROFESSION], [53, 67, ORGANIZATION],...</td>\n",
       "      <td>Умер избитый до комы гитарист и сооснователь г...</td>\n",
       "      <td>1</td>\n",
       "      <td>[(Умер, 0, 3), (избитый, 5, 11), (до, 13, 14),...</td>\n",
       "      <td>[(0, 3), (5, 11), (13, 14), (16, 19), (21, 28)...</td>\n",
       "      <td>[((4, 4), (21, 28)), ((9, 10), (53, 67)), ((16...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0, 4, PERSON], [37, 42, COUNTRY], [47, 76, O...</td>\n",
       "      <td>Путин подписал распоряжение о выходе России из...</td>\n",
       "      <td>2</td>\n",
       "      <td>[(Путин, 0, 4), (подписал, 6, 13), (распоряжен...</td>\n",
       "      <td>[(0, 4), (6, 13), (15, 26), (28, 28), (30, 35)...</td>\n",
       "      <td>[((0, 0), (0, 4)), ((5, 5), (37, 42)), ((7, 9)...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0, 11, PERSON], [36, 47, PROFESSION], [49, 6...</td>\n",
       "      <td>Бенедикт XVI носил кардиостимулятор\\nПапа Римс...</td>\n",
       "      <td>3</td>\n",
       "      <td>[(Бенедикт, 0, 7), (XVI, 9, 11), (носил, 13, 1...</td>\n",
       "      <td>[(0, 7), (9, 11), (13, 17), (19, 34), (36, 39)...</td>\n",
       "      <td>[((0, 1), (0, 11)), ((4, 5), (36, 47)), ((6, 7...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0, 4, PERSON], [17, 29, ORGANIZATION], [48, ...</td>\n",
       "      <td>Обама назначит в Верховный суд латиноамериканк...</td>\n",
       "      <td>4</td>\n",
       "      <td>[(Обама, 0, 4), (назначит, 6, 13), (в, 15, 15)...</td>\n",
       "      <td>[(0, 4), (6, 13), (15, 15), (17, 25), (27, 29)...</td>\n",
       "      <td>[((0, 0), (0, 4)), ((3, 4), (17, 29)), ((6, 6)...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>[[42, 46, COUNTRY], [82, 87, COUNTRY], [104, 1...</td>\n",
       "      <td>Глава Малайзии: мы не хотим противостоять Кита...</td>\n",
       "      <td>514</td>\n",
       "      <td>[(Глава, 0, 4), (Малайзии, 6, 13), (:, 14, 14)...</td>\n",
       "      <td>[(0, 4), (6, 13), (14, 14), (16, 17), (19, 20)...</td>\n",
       "      <td>[((7, 7), (42, 46)), ((13, 13), (82, 87)), ((1...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>[[1, 4, PRODUCT], [31, 33, FACILITY], [35, 44,...</td>\n",
       "      <td>\"Союз\" впервые пристыковался к МКС за 6 часов\\...</td>\n",
       "      <td>515</td>\n",
       "      <td>[(``, 0, 0), (Союз, 1, 4), ('', 5, 5), (впервы...</td>\n",
       "      <td>[(0, 0), (1, 4), (5, 5), (7, 13), (15, 27), (2...</td>\n",
       "      <td>[((1, 1), (1, 4)), ((6, 6), (31, 33)), ((7, 9)...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>[[0, 4, PERSON], [8, 12, PERSON], [45, 52, AGE...</td>\n",
       "      <td>Трамп и Путин сделали совместное заявление к 7...</td>\n",
       "      <td>516</td>\n",
       "      <td>[(Трамп, 0, 4), (и, 6, 6), (Путин, 8, 12), (сд...</td>\n",
       "      <td>[(0, 4), (6, 6), (8, 12), (14, 20), (22, 31), ...</td>\n",
       "      <td>[((0, 0), (0, 4)), ((2, 2), (8, 12)), ((7, 9),...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>[[0, 9, NATIONALITY], [58, 72, PERSON], [101, ...</td>\n",
       "      <td>Российский магнат устроил самую дорогую свадьб...</td>\n",
       "      <td>517</td>\n",
       "      <td>[(Российский, 0, 9), (магнат, 11, 16), (устрои...</td>\n",
       "      <td>[(0, 9), (11, 16), (18, 24), (26, 30), (32, 38...</td>\n",
       "      <td>[((0, 0), (0, 9)), ((8, 9), (58, 72)), ((12, 1...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>[[0, 4, PERSON], [16, 25, PROFESSION], [27, 38...</td>\n",
       "      <td>Трамп поздравил астронавта Пегги Уитсон с уста...</td>\n",
       "      <td>518</td>\n",
       "      <td>[(Трамп, 0, 4), (поздравил, 6, 14), (астронавт...</td>\n",
       "      <td>[(0, 4), (6, 14), (16, 25), (27, 31), (33, 38)...</td>\n",
       "      <td>[((0, 0), (0, 4)), ((2, 2), (16, 25)), ((3, 4)...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>519 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  ners  \\\n",
       "0    [[0, 5, CITY], [16, 23, PERSON], [34, 41, PERS...   \n",
       "1    [[21, 28, PROFESSION], [53, 67, ORGANIZATION],...   \n",
       "2    [[0, 4, PERSON], [37, 42, COUNTRY], [47, 76, O...   \n",
       "3    [[0, 11, PERSON], [36, 47, PROFESSION], [49, 6...   \n",
       "4    [[0, 4, PERSON], [17, 29, ORGANIZATION], [48, ...   \n",
       "..                                                 ...   \n",
       "514  [[42, 46, COUNTRY], [82, 87, COUNTRY], [104, 1...   \n",
       "515  [[1, 4, PRODUCT], [31, 33, FACILITY], [35, 44,...   \n",
       "516  [[0, 4, PERSON], [8, 12, PERSON], [45, 52, AGE...   \n",
       "517  [[0, 9, NATIONALITY], [58, 72, PERSON], [101, ...   \n",
       "518  [[0, 4, PERSON], [16, 25, PROFESSION], [27, 38...   \n",
       "\n",
       "                                             sentences   id  \\\n",
       "0    Бостон взорвали Тамерлан и Джохар Царнаевы из ...    0   \n",
       "1    Умер избитый до комы гитарист и сооснователь г...    1   \n",
       "2    Путин подписал распоряжение о выходе России из...    2   \n",
       "3    Бенедикт XVI носил кардиостимулятор\\nПапа Римс...    3   \n",
       "4    Обама назначит в Верховный суд латиноамериканк...    4   \n",
       "..                                                 ...  ...   \n",
       "514  Глава Малайзии: мы не хотим противостоять Кита...  514   \n",
       "515  \"Союз\" впервые пристыковался к МКС за 6 часов\\...  515   \n",
       "516  Трамп и Путин сделали совместное заявление к 7...  516   \n",
       "517  Российский магнат устроил самую дорогую свадьб...  517   \n",
       "518  Трамп поздравил астронавта Пегги Уитсон с уста...  518   \n",
       "\n",
       "                                                tokens  \\\n",
       "0    [(Бостон, 0, 5), (взорвали, 7, 14), (Тамерлан,...   \n",
       "1    [(Умер, 0, 3), (избитый, 5, 11), (до, 13, 14),...   \n",
       "2    [(Путин, 0, 4), (подписал, 6, 13), (распоряжен...   \n",
       "3    [(Бенедикт, 0, 7), (XVI, 9, 11), (носил, 13, 1...   \n",
       "4    [(Обама, 0, 4), (назначит, 6, 13), (в, 15, 15)...   \n",
       "..                                                 ...   \n",
       "514  [(Глава, 0, 4), (Малайзии, 6, 13), (:, 14, 14)...   \n",
       "515  [(``, 0, 0), (Союз, 1, 4), ('', 5, 5), (впервы...   \n",
       "516  [(Трамп, 0, 4), (и, 6, 6), (Путин, 8, 12), (сд...   \n",
       "517  [(Российский, 0, 9), (магнат, 11, 16), (устрои...   \n",
       "518  [(Трамп, 0, 4), (поздравил, 6, 14), (астронавт...   \n",
       "\n",
       "                                                 spans  \\\n",
       "0    [(0, 5), (7, 14), (16, 23), (25, 25), (27, 32)...   \n",
       "1    [(0, 3), (5, 11), (13, 14), (16, 19), (21, 28)...   \n",
       "2    [(0, 4), (6, 13), (15, 26), (28, 28), (30, 35)...   \n",
       "3    [(0, 7), (9, 11), (13, 17), (19, 34), (36, 39)...   \n",
       "4    [(0, 4), (6, 13), (15, 15), (17, 25), (27, 29)...   \n",
       "..                                                 ...   \n",
       "514  [(0, 4), (6, 13), (14, 14), (16, 17), (19, 20)...   \n",
       "515  [(0, 0), (1, 4), (5, 5), (7, 13), (15, 27), (2...   \n",
       "516  [(0, 4), (6, 6), (8, 12), (14, 20), (22, 31), ...   \n",
       "517  [(0, 9), (11, 16), (18, 24), (26, 30), (32, 38...   \n",
       "518  [(0, 4), (6, 14), (16, 25), (27, 31), (33, 38)...   \n",
       "\n",
       "                                             span_ners  \\\n",
       "0    [((0, 0), (0, 5)), ((2, 2), (16, 23)), ((5, 5)...   \n",
       "1    [((4, 4), (21, 28)), ((9, 10), (53, 67)), ((16...   \n",
       "2    [((0, 0), (0, 4)), ((5, 5), (37, 42)), ((7, 9)...   \n",
       "3    [((0, 1), (0, 11)), ((4, 5), (36, 47)), ((6, 7...   \n",
       "4    [((0, 0), (0, 4)), ((3, 4), (17, 29)), ((6, 6)...   \n",
       "..                                                 ...   \n",
       "514  [((7, 7), (42, 46)), ((13, 13), (82, 87)), ((1...   \n",
       "515  [((1, 1), (1, 4)), ((6, 6), (31, 33)), ((7, 9)...   \n",
       "516  [((0, 0), (0, 4)), ((2, 2), (8, 12)), ((7, 9),...   \n",
       "517  [((0, 0), (0, 9)), ((8, 9), (58, 72)), ((12, 1...   \n",
       "518  [((0, 0), (0, 4)), ((2, 2), (16, 25)), ((3, 4)...   \n",
       "\n",
       "                                                labels  \n",
       "0    [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "1    [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "2    [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "3    [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "4    [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "..                                                 ...  \n",
       "514  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "515  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "516  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "517  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...  \n",
       "518  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "\n",
       "[519 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a class for preprocessing Russian Named Entity Recognition (NER) data\n",
    "\n",
    "class RuNNEPreprocessor:\n",
    "    def __init__(self, DATASET_PATH=Path(\"public_dat\")):\n",
    "        # Load and preprocess the training dataset\n",
    "        self.train = pd.read_json(DATASET_PATH / 'train.jsonl', lines=True)\n",
    "        self.train['sentences'] = self.train.sentences.apply(lambda x: x.replace('«', '\\\"').replace('»', '\\\"'))\n",
    "\n",
    "    # Function to get the positions of tokens in a sentence\n",
    "    def get_pos(self, sent: str, tokens: list[str]):\n",
    "        pos = []\n",
    "        start = 0\n",
    "        for token in tokens:\n",
    "            t_start = sent.find(token, start)\n",
    "            if t_start != -1:\n",
    "                pos.append((t_start, t_start + len(token) - 1))\n",
    "                start = t_start + len(token)\n",
    "        return pos\n",
    "    \n",
    "    # Preprocess labels for NER\n",
    "    def label_preprocessing(self):\n",
    "        self.all_labels = set(self.train.ners.apply(lambda x: [y[2] for y in x]).sum())\n",
    "        self.dual_labels = {'B-' + x for x in self.all_labels}.union({'I-' + x for x in self.all_labels})\n",
    "        print(len(self.all_labels), self.all_labels)\n",
    "        self.label2id = {v: k for k, v in enumerate(self.dual_labels)}\n",
    "        self.id2label = {k: v for k, v in enumerate(self.dual_labels)}\n",
    "    \n",
    "    # Tokenize sentences and map NER labels to token spans\n",
    "    def tokenizer_transform(self):\n",
    "\n",
    "        tokenizer = TreebankWordTokenizer()\n",
    "        punctuation_patterns = [\n",
    "            (re.compile(r'(\\w)\\.'), '\\1 \\.'),\n",
    "            (re.compile(r'([^\\s])(-|\\|)([^\\s])'), r'\\1 \\2 \\3'),\n",
    "            (re.compile(r'(\\d+):(\\d)+'), r'\\1 : \\2')\n",
    "        ]\n",
    "\n",
    "        for pattern, replacement in punctuation_patterns:\n",
    "            if (pattern, replacement) not in tokenizer.PUNCTUATION:\n",
    "                tokenizer.PUNCTUATION.append((pattern, replacement))\n",
    "\n",
    "        def tokenize_(sentence, tokenizer):\n",
    "            tokens_ = tokenizer.tokenize(sentence)\n",
    "            spans_ = tokenizer.span_tokenize(sentence)\n",
    "\n",
    "            tokens_with_spans = [(token, start, end - 1) for token, (start, end) in zip(tokens_, spans_)]\n",
    "\n",
    "            return tokens_with_spans\n",
    "        def get_spans(sentence, tokenizer):\n",
    "            spans_ = tokenizer.span_tokenize(sentence)\n",
    "            spans = [(start, end - 1) for start, end in spans_]\n",
    "            return spans\n",
    "        \n",
    "        self.train['tokens'] = self.train.sentences.apply(lambda x: tokenize_(x, tokenizer))\n",
    "        self.train['spans'] = self.train.sentences.apply(lambda x: get_spans(x, tokenizer))\n",
    "\n",
    "    # Map NER labels to token spans\n",
    "    def map_ner(self, ner: tuple[int, int, str], spans: list[tuple[int, int]]):\n",
    "        def left_binary_search(spans, ner):\n",
    "            left, right = 0, len(spans)\n",
    "            while right - left > 1:\n",
    "                mid = (right + left) >> 1\n",
    "                if spans[mid][0] <= ner[0]:\n",
    "                    left = mid\n",
    "                else:\n",
    "                    right = mid\n",
    "\n",
    "            return left\n",
    "        \n",
    "        def right_binary_search(spans, ner):\n",
    "            left, right = -1, len(spans) -1\n",
    "            while right - left > 1:\n",
    "                mid = (right + left) >> 1\n",
    "                if spans[mid][1] >= ner[1]:\n",
    "                    right = mid\n",
    "                else:\n",
    "                    left = mid\n",
    "            return right\n",
    "\n",
    "        left_span = left_binary_search(spans, ner)\n",
    "        right_span = right_binary_search(spans, ner)\n",
    "        \n",
    "        assert left_span <= right_span, (left_span, spans[left_span], right_span, spans[right_span], ner)\n",
    "        return (left_span, right_span), (spans[left_span][0], spans[right_span][1])\n",
    "        \n",
    "    # Span NER labels across tokens\n",
    "    def span_ners(self):\n",
    "        self.train['span_ners'] = self.train.apply(lambda x: [self.map_ner(y, x['spans']) for y in x['ners']], axis=1)\n",
    "\n",
    "    # Create labels for each token based on NER spans\n",
    "    def create_labels(self):\n",
    "        def create_labels_(ners, spans):\n",
    "            labels = [[0 for _ in self.dual_labels] for _ in spans]\n",
    "            for n in ners:\n",
    "                (i, j), _ = self.map_ner(n, spans)\n",
    "                labels[i][self.label2id['B-' + n[2]]] = 1\n",
    "                for k in range(i + 1, j + 1):\n",
    "                    labels[k][self.label2id['I-' + n[2]]] = 1\n",
    "            return labels\n",
    "\n",
    "        def label_tokens(row):\n",
    "            return create_labels_(row.ners, row.spans)\n",
    "\n",
    "        self.train['labels'] = self.train.apply(label_tokens, axis=1)\n",
    "\n",
    "    # Preprocess data by labeling and tokenizing\n",
    "    def preprocess_data(self):\n",
    "        self.label_preprocessing()\n",
    "        self.tokenizer_transform()\n",
    "        self.span_ners()\n",
    "        self.create_labels()\n",
    "        return self.train\n",
    "\n",
    "# Instantiate the preprocessor and preprocess the data\n",
    "preprocessor = RuNNEPreprocessor()\n",
    "train = preprocessor.preprocess_data()\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29,\n",
       " {'AGE',\n",
       "  'AWARD',\n",
       "  'CITY',\n",
       "  'COUNTRY',\n",
       "  'CRIME',\n",
       "  'DATE',\n",
       "  'DISEASE',\n",
       "  'DISTRICT',\n",
       "  'EVENT',\n",
       "  'FACILITY',\n",
       "  'FAMILY',\n",
       "  'IDEOLOGY',\n",
       "  'LANGUAGE',\n",
       "  'LAW',\n",
       "  'LOCATION',\n",
       "  'MONEY',\n",
       "  'NATIONALITY',\n",
       "  'NUMBER',\n",
       "  'ORDINAL',\n",
       "  'ORGANIZATION',\n",
       "  'PENALTY',\n",
       "  'PERCENT',\n",
       "  'PERSON',\n",
       "  'PRODUCT',\n",
       "  'PROFESSION',\n",
       "  'RELIGION',\n",
       "  'STATE_OR_PROVINCE',\n",
       "  'TIME',\n",
       "  'WORK_OF_ART'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract all unique labels and dual labels for NER\n",
    "all_labels = set(train.ners.apply(lambda x: [y[2] for y in x]).sum())\n",
    "len(all_labels), all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dual labels for B- and I- prefixes\n",
    "dual_labels = {'B-' + x for x in all_labels}.union({'I-' + x for x in all_labels})\n",
    "\n",
    "# Create dictionaries for label to ID and ID to label mappings\n",
    "label2id = {\n",
    "    v: k\n",
    "    for k, v in enumerate(dual_labels)\n",
    "}\n",
    "id2label = {\n",
    "    k: v\n",
    "    for k, v in enumerate(dual_labels)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import re\n",
    "\n",
    "# Tokenize sentences and adjust punctuation patterns\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "punctuation_patterns = [\n",
    "    (re.compile(r'(\\w)\\.'), '\\1 \\.'),\n",
    "    (re.compile(r'([^\\s])(-|\\|)([^\\s])'), r'\\1 \\2 \\3'),\n",
    "    (re.compile(r'(\\d+):(\\d)+'), r'\\1 : \\2')\n",
    "]\n",
    "\n",
    "for pattern, replacement in punctuation_patterns:\n",
    "    if (pattern, replacement) not in tokenizer.PUNCTUATION:\n",
    "        tokenizer.PUNCTUATION.append((pattern, replacement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135505, 135505)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the dataset for training\n",
    "t = train.copy()\n",
    "t.tokens = t.tokens.apply(lambda x: [y[0] for y in x])\n",
    "t = t[['tokens', 'labels']]\n",
    "\n",
    "all_tokens = t.tokens.sum()\n",
    "all_labels = t.labels.sum()\n",
    "\n",
    "len(all_tokens), len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135473, 135473)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define window size for sequence creation\n",
    "WINDOW = 32\n",
    "\n",
    "# Create sequences and labels for training\n",
    "sequences, labels = [], []\n",
    "\n",
    "for t in range(len(all_tokens) - WINDOW):\n",
    "    sequences.append(all_tokens[t:t+WINDOW])\n",
    "    labels.append(all_labels[t:t+WINDOW])\n",
    "    \n",
    "len(sequences), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Бостон, взорвали, Тамерлан, и, Джохар, Царнае...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[взорвали, Тамерлан, и, Джохар, Царнаевы, из, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Тамерлан, и, Джохар, Царнаевы, из, Северного,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[и, Джохар, Царнаевы, из, Северного, Кавказа, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Джохар, Царнаевы, из, Северного, Кавказа, 19,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [Бостон, взорвали, Тамерлан, и, Джохар, Царнае...   \n",
       "1  [взорвали, Тамерлан, и, Джохар, Царнаевы, из, ...   \n",
       "2  [Тамерлан, и, Джохар, Царнаевы, из, Северного,...   \n",
       "3  [и, Джохар, Царнаевы, из, Северного, Кавказа, ...   \n",
       "4  [Джохар, Царнаевы, из, Северного, Кавказа, 19,...   \n",
       "\n",
       "                                               label  \n",
       "0  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "1  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,...  \n",
       "3  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "4  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched = pd.DataFrame([*zip(sequences, labels)], columns=['tokens', 'label'])\n",
    "batched.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121926, 13547, 519)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_split = batched.sample(frac=0.9)\n",
    "val_split = batched[~batched.index.isin(train_split.index)]\n",
    "\n",
    "len(train_split), len(val_split), len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_label = train_split.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import DatasetDict, Dataset\n",
    "\n",
    "# dataset = DatasetDict({\n",
    "#     'train': Dataset.from_pandas(train_split.reset_index(drop=True)),\n",
    "#     'val': Dataset.from_pandas(val_split.reset_index(drop=True))\n",
    "# })\n",
    "\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\schir\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertModel, BertConfig\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a custom model for NER using BERT\n",
    "class BertForNER(BertModel):\n",
    "    def __init__(self, config: BertConfig):\n",
    "        super().__init__(config)\n",
    "        # Initialize a linear layer for classification with the number of output classes\n",
    "        self.classifier_head = nn.Linear(config.hidden_size, 58)\n",
    "        # Define a tensor for positive weights to handle class imbalance in the loss function\n",
    "        self.__pos_weight = torch.full((1, 1, 58), 58)\n",
    "        \n",
    "    def forward(self, return_loss = True, **kwargs):\n",
    "        # Extract labels from the keyword arguments        \n",
    "        labels = kwargs.pop('labels', None)\n",
    "        # Remove 'output_hidden_states' from kwargs if present\n",
    "        kwargs.pop('output_hidden_states', None)\n",
    "        \n",
    "        # Forward pass through the BERT model\n",
    "        output = super().forward(**kwargs, return_dict=True, output_hidden_states=True)\n",
    "        # Apply the classifier head to the last hidden state\n",
    "        preds = self.classifier_head(output.hidden_states[-1])\n",
    "        \n",
    "        # Add predictions to the output dictionary\n",
    "        output['predictions'] = preds\n",
    "        if labels is not None:\n",
    "            # Calculate the binary cross-entropy loss with logits, using the positive weights\n",
    "            loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "                preds,\n",
    "                labels,\n",
    "                pos_weight=self.__pos_weight.to(preds.device)\n",
    "            )     \n",
    "        else:\n",
    "            loss = None       \n",
    "\n",
    "        # Return the loss and output if labels are provided and loss is requested, otherwise return only the output\n",
    "        return loss, output if labels is not None and return_loss else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForNER: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForNER from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNER from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForNER were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['bert.classifier_head.bias', 'bert.classifier_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained BERT model for NER\n",
    "model = BertForNER.from_pretrained('DeepPavlov/rubert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare the dataset for training by converting tokens to IDs and labels to tensors\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    tokens = [bert_tokenizer.convert_tokens_to_ids(x) for x in batch['tokens']]\n",
    "        \n",
    "    input_ids = torch.tensor(tokens, dtype=torch.long)\n",
    "    labels = torch.tensor(batch['label'], dtype=torch.float32)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'labels': labels\n",
    "    }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/121926 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 121926/121926 [09:06<00:00, 223.05 examples/s]\n",
      "Map: 100%|██████████| 13547/13547 [01:26<00:00, 156.45 examples/s]\n",
      "Saving the dataset (6/6 shards): 100%|██████████| 121926/121926 [00:06<00:00, 18711.70 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 13547/13547 [00:00<00:00, 37625.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# dataset.map(prepare_dataset, batched=True, batch_size=256).remove_columns('tokens').save_to_disk('./dataset/tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenized dataset from disk\n",
    "from datasets import DatasetDict\n",
    "\n",
    "ds = DatasetDict.load_from_disk('./dataset/tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.remove_columns('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1,2,3\n"
     ]
    }
   ],
   "source": [
    "# Set the environment variable to specify which GPUs to use\n",
    "%env CUDA_VISIBLE_DEVICES=1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Function to compute metrics such as accuracy from the model outputs\n",
    "def compute_metrics(outputs):\n",
    "    predictions: np.ndarray = outputs.predicitions\n",
    "    labels: np.ndarray = outputs.labels\n",
    "    \n",
    "    predictions = predictions.flatten()\n",
    "    labels = predictions.flatten()\n",
    "    accuracy = ((predictions == 1) & (labels == 1)).sum() / max(1, (predictions == 1).sum())\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13547/13547 [25:05<00:00,  9.00it/s]\n",
      "100%|██████████| 13547/13547 [28:49<00:00,  7.83it/s]\n",
      "100%|██████████| 13547/13547 [25:16<00:00,  8.93it/s]\n",
      "100%|██████████| 3/3 [1:19:11<00:00, 1583.96s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.4728), tensor(0.0082), tensor(nan))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Set the device to use for training\n",
    "device = 'cuda:7' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "epochs = 3\n",
    "accuracy, recall, precision, n = 0, 0, 0, 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for batch in tqdm(ds['val']):\n",
    "        tokens = torch.tensor(batch['input_ids'], dtype=torch.long, device=device).unsqueeze(0)\n",
    "        labels = torch.tensor(batch['labels'], dtype=torch.long, device=device).unsqueeze(0)\n",
    "        outputs = model(input_ids=tokens)[1].predictions\n",
    "\n",
    "        # Convert logits to probabilities and apply threshold to get predictions\n",
    "        tags = (torch.nn.functional.sigmoid(outputs).squeeze() > .5).to(torch.long)\n",
    "        \n",
    "        # Calculate true positives, true negatives, false positives, and false negatives\n",
    "        tp = ((labels == 1) & (tags == 1)).sum()\n",
    "        tn = ((labels == 0) & (tags == 0)).sum()\n",
    "        fp = ((labels == 0) & (tags == 1)).sum()\n",
    "        fn = ((labels == 1) & (tags == 0)).sum()\n",
    "        \n",
    "        # Update precision, recall, and accuracy\n",
    "        precision += tp / (tp + fp)\n",
    "        recall += tp / (tp + fn)\n",
    "        accuracy += (tp + fp) / labels.numel()\n",
    "        n += 1\n",
    "\n",
    "# Calculate average accuracy, precision, and recall\n",
    "accuracy / n, precision / n, recall / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Function to convert model predictions to a submission format\n",
    "def convert_to_submit(labels: list[torch.Tensor], spans: list[tuple[int, int]]):\n",
    "    # labels of shape (sequence_length, num_classes), binary tensor (0 or 1)\n",
    "    # spans are pairs (begin, end)\n",
    "    # assert len(labels) == len(spans)\n",
    "    \n",
    "    # Initialize sets for start and segment label IDs\n",
    "    start_label_ids = {v for k, v in label2id.items() if k.startswith('B-')}\n",
    "    \n",
    "    ners = []\n",
    "    current_ners = []\n",
    "    for i, label in enumerate(labels):\n",
    "        index = torch.arange(58)\n",
    "        predicted = index[label == 1].tolist()\n",
    "        expanded = [False] * len(current_ners)\n",
    "        new_ners = []\n",
    "        for p in predicted:\n",
    "            if p not in start_label_ids:\n",
    "                for j, c in enumerate(current_ners):\n",
    "                    if c[0] == id2label[p][2:]:\n",
    "                        # expanding\n",
    "                        c = (c[0], c[1], spans[i][1])\n",
    "                        expanded[j] = True\n",
    "            else:\n",
    "                new_ners.append((id2label[p][2:], *spans[i]))\n",
    "        \n",
    "        ners.extend([(c[1], c[2], c[0]) for j, c in enumerate(current_ners) if not expanded[j]])\n",
    "        current_ners = [c for j, c in enumerate(current_ners) if expanded[j]]\n",
    "        current_ners += new_ners\n",
    "        \n",
    "    return ners\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Бостон взорвали Тамерлан и Джохар Царнаевы из Северного Кавказа\\n\\n19 апреля 2013 года в пригороде Бостона  проходит спецоперация по поимке 19-летнего Джохара Царнаева, подозреваемого в теракте на Бостонском марафоне 15 апреля и в смертельном ранении полицей',\n",
       " ['Бостон',\n",
       "  'взорвали',\n",
       "  'Тамерлан',\n",
       "  'и',\n",
       "  'Джохар',\n",
       "  'Царнаевы',\n",
       "  'из',\n",
       "  'Северного',\n",
       "  'Кавказа',\n",
       "  '19',\n",
       "  'апреля',\n",
       "  '2013',\n",
       "  'года',\n",
       "  'в',\n",
       "  'пригороде',\n",
       "  'Бостона',\n",
       "  'проходит',\n",
       "  'спецоперация',\n",
       "  'по',\n",
       "  'поимке',\n",
       "  '19',\n",
       "  '-',\n",
       "  'летнего',\n",
       "  'Джохара',\n",
       "  'Царнаева',\n",
       "  ',',\n",
       "  'подозреваемого',\n",
       "  'в',\n",
       "  'теракте',\n",
       "  'на',\n",
       "  'Бостонском',\n",
       "  'марафоне',\n",
       "  '15',\n",
       "  'апреля',\n",
       "  'и',\n",
       "  'в',\n",
       "  'смертельном',\n",
       "  'ранении',\n",
       "  'полицей'],\n",
       " [(0, 6),\n",
       "  (7, 15),\n",
       "  (16, 24),\n",
       "  (25, 26),\n",
       "  (27, 33),\n",
       "  (34, 42),\n",
       "  (43, 45),\n",
       "  (46, 55),\n",
       "  (56, 63),\n",
       "  (65, 67),\n",
       "  (68, 74),\n",
       "  (75, 79),\n",
       "  (80, 84),\n",
       "  (85, 86),\n",
       "  (87, 96),\n",
       "  (97, 104),\n",
       "  (106, 114),\n",
       "  (115, 127),\n",
       "  (128, 130),\n",
       "  (131, 137),\n",
       "  (138, 140),\n",
       "  (140, 141),\n",
       "  (141, 148),\n",
       "  (149, 156),\n",
       "  (157, 165),\n",
       "  (165, 166),\n",
       "  (167, 181),\n",
       "  (182, 183),\n",
       "  (184, 191),\n",
       "  (192, 194),\n",
       "  (195, 205),\n",
       "  (206, 214),\n",
       "  (215, 217),\n",
       "  (218, 224),\n",
       "  (225, 226),\n",
       "  (227, 228),\n",
       "  (229, 240),\n",
       "  (241, 248),\n",
       "  (249, 256)],\n",
       " [37104,\n",
       "  65193,\n",
       "  82820,\n",
       "  851,\n",
       "  100,\n",
       "  100,\n",
       "  1703,\n",
       "  19939,\n",
       "  27083,\n",
       "  1653,\n",
       "  6167,\n",
       "  7262,\n",
       "  1768,\n",
       "  845,\n",
       "  30319,\n",
       "  58359,\n",
       "  12455,\n",
       "  90484,\n",
       "  1516,\n",
       "  68780,\n",
       "  1653,\n",
       "  130,\n",
       "  17365,\n",
       "  100,\n",
       "  100,\n",
       "  128,\n",
       "  33503,\n",
       "  845,\n",
       "  61965,\n",
       "  1469,\n",
       "  100,\n",
       "  61263,\n",
       "  3996,\n",
       "  6167,\n",
       "  851,\n",
       "  845,\n",
       "  100,\n",
       "  100,\n",
       "  12707])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = train.sentences.iloc[0][:256]\n",
    "tokens = tokenizer.tokenize(text)\n",
    "spans = [*tokenizer.span_tokenize(text)]\n",
    "bert_tokens = list(map(bert_tokenizer.convert_tokens_to_ids, tokens))\n",
    "\n",
    "text, tokens, spans, bert_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 39, 58])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(input_ids=torch.tensor(bert_tokens, dtype=torch.long).unsqueeze(0))[1].predictions\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 39, 58])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = (torch.nn.functional.sigmoid(outputs) > 0.69).to(torch.long)\n",
    "tags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(36)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tags == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([291, 58]),\n",
       " [(0, 6),\n",
       "  (7, 15),\n",
       "  (16, 23),\n",
       "  (24, 31),\n",
       "  (32, 36),\n",
       "  (37, 38),\n",
       "  (39, 45),\n",
       "  (46, 49),\n",
       "  (50, 57),\n",
       "  (58, 62),\n",
       "  (63, 72),\n",
       "  (73, 79),\n",
       "  (80, 82),\n",
       "  (83, 88),\n",
       "  (89, 98),\n",
       "  (99, 104),\n",
       "  (105, 107),\n",
       "  (108, 115),\n",
       "  (116, 117),\n",
       "  (118, 124),\n",
       "  (124, 125),\n",
       "  (126, 134),\n",
       "  (135, 139),\n",
       "  (140, 144),\n",
       "  (145, 146),\n",
       "  (147, 153),\n",
       "  (154, 162),\n",
       "  (163, 169),\n",
       "  (170, 185),\n",
       "  (186, 192),\n",
       "  (193, 200),\n",
       "  (201, 206),\n",
       "  (206, 207),\n",
       "  (208, 220),\n",
       "  (221, 226),\n",
       "  (227, 236),\n",
       "  (237, 243),\n",
       "  (243, 244),\n",
       "  (245, 256),\n",
       "  (257, 262),\n",
       "  (263, 270),\n",
       "  (271, 278),\n",
       "  (279, 285),\n",
       "  (286, 294),\n",
       "  (294, 295),\n",
       "  (297, 304),\n",
       "  (305, 311),\n",
       "  (312, 313),\n",
       "  (314, 321),\n",
       "  (322, 326),\n",
       "  (327, 337),\n",
       "  (338, 339),\n",
       "  (340, 347),\n",
       "  (347, 348),\n",
       "  (349, 354),\n",
       "  (355, 363),\n",
       "  (364, 366),\n",
       "  (367, 380),\n",
       "  (381, 390),\n",
       "  (391, 399),\n",
       "  (400, 401),\n",
       "  (402, 404),\n",
       "  (405, 412),\n",
       "  (412, 413),\n",
       "  (414, 415),\n",
       "  (416, 424),\n",
       "  (425, 426),\n",
       "  (427, 432),\n",
       "  (432, 433),\n",
       "  (433, 439),\n",
       "  (440, 451),\n",
       "  (452, 456),\n",
       "  (456, 457),\n",
       "  (458, 468),\n",
       "  (469, 476),\n",
       "  (477, 483),\n",
       "  (484, 491),\n",
       "  (492, 497),\n",
       "  (498, 507),\n",
       "  (508, 518),\n",
       "  (519, 525),\n",
       "  (526, 531),\n",
       "  (531, 532),\n",
       "  (533, 536),\n",
       "  (537, 542),\n",
       "  (543, 552),\n",
       "  (553, 562),\n",
       "  (563, 571),\n",
       "  (571, 572),\n",
       "  (573, 582),\n",
       "  (583, 584),\n",
       "  (585, 586),\n",
       "  (586, 587),\n",
       "  (588, 597),\n",
       "  (597, 598),\n",
       "  (599, 600),\n",
       "  (600, 601),\n",
       "  (602, 612),\n",
       "  (613, 614),\n",
       "  (615, 616),\n",
       "  (616, 617),\n",
       "  (618, 624),\n",
       "  (624, 625),\n",
       "  (626, 632),\n",
       "  (633, 639),\n",
       "  (640, 649),\n",
       "  (649, 650),\n",
       "  (651, 652),\n",
       "  (653, 659),\n",
       "  (660, 667),\n",
       "  (668, 678),\n",
       "  (679, 688),\n",
       "  (688, 689),\n",
       "  (690, 697),\n",
       "  (698, 707),\n",
       "  (707, 708),\n",
       "  (709, 715),\n",
       "  (716, 720),\n",
       "  (721, 722),\n",
       "  (723, 727),\n",
       "  (728, 731),\n",
       "  (731, 732),\n",
       "  (734, 745),\n",
       "  (746, 751),\n",
       "  (752, 759),\n",
       "  (760, 763),\n",
       "  (764, 768),\n",
       "  (769, 773),\n",
       "  (774, 776),\n",
       "  (777, 782),\n",
       "  (783, 790),\n",
       "  (791, 795),\n",
       "  (795, 796),\n",
       "  (797, 799),\n",
       "  (800, 806),\n",
       "  (806, 807),\n",
       "  (808, 817),\n",
       "  (818, 824),\n",
       "  (825, 826),\n",
       "  (827, 832),\n",
       "  (833, 839),\n",
       "  (840, 849),\n",
       "  (850, 863),\n",
       "  (864, 866),\n",
       "  (866, 867),\n",
       "  (867, 873),\n",
       "  (874, 883),\n",
       "  (884, 891),\n",
       "  (891, 892),\n",
       "  (893, 895),\n",
       "  (896, 900),\n",
       "  (901, 902),\n",
       "  (903, 904),\n",
       "  (905, 910),\n",
       "  (911, 915),\n",
       "  (916, 926),\n",
       "  (927, 937),\n",
       "  (938, 944),\n",
       "  (944, 945),\n",
       "  (946, 955),\n",
       "  (956, 957),\n",
       "  (958, 975),\n",
       "  (976, 982),\n",
       "  (982, 983),\n",
       "  (984, 992),\n",
       "  (993, 998),\n",
       "  (999, 1011),\n",
       "  (1011, 1012),\n",
       "  (1013, 1016),\n",
       "  (1017, 1019),\n",
       "  (1020, 1026),\n",
       "  (1027, 1031),\n",
       "  (1032, 1038),\n",
       "  (1039, 1046),\n",
       "  (1047, 1053),\n",
       "  (1053, 1054),\n",
       "  (1054, 1063),\n",
       "  (1063, 1064),\n",
       "  (1065, 1066),\n",
       "  (1067, 1072),\n",
       "  (1073, 1083),\n",
       "  (1084, 1096),\n",
       "  (1097, 1104),\n",
       "  (1104, 1105),\n",
       "  (1106, 1107),\n",
       "  (1108, 1115),\n",
       "  (1116, 1120),\n",
       "  (1121, 1135),\n",
       "  (1136, 1137),\n",
       "  (1138, 1148),\n",
       "  (1149, 1158),\n",
       "  (1158, 1159),\n",
       "  (1161, 1168),\n",
       "  (1169, 1178),\n",
       "  (1179, 1180),\n",
       "  (1181, 1194),\n",
       "  (1194, 1195),\n",
       "  (1196, 1200),\n",
       "  (1201, 1208),\n",
       "  (1209, 1220),\n",
       "  (1221, 1230),\n",
       "  (1231, 1241),\n",
       "  (1242, 1251),\n",
       "  (1251, 1252),\n",
       "  (1253, 1260),\n",
       "  (1261, 1264),\n",
       "  (1265, 1271),\n",
       "  (1272, 1281),\n",
       "  (1282, 1288),\n",
       "  (1289, 1290),\n",
       "  (1291, 1298),\n",
       "  (1299, 1302),\n",
       "  (1303, 1311),\n",
       "  (1312, 1313),\n",
       "  (1314, 1316),\n",
       "  (1317, 1322),\n",
       "  (1322, 1323),\n",
       "  (1325, 1334),\n",
       "  (1335, 1341),\n",
       "  (1341, 1342),\n",
       "  (1343, 1347),\n",
       "  (1347, 1348),\n",
       "  (1348, 1360),\n",
       "  (1361, 1363),\n",
       "  (1364, 1375),\n",
       "  (1375, 1376),\n",
       "  (1377, 1381),\n",
       "  (1382, 1390),\n",
       "  (1391, 1396),\n",
       "  (1397, 1410),\n",
       "  (1411, 1412),\n",
       "  (1413, 1430),\n",
       "  (1431, 1444),\n",
       "  (1444, 1445),\n",
       "  (1446, 1447),\n",
       "  (1448, 1452),\n",
       "  (1453, 1457),\n",
       "  (1458, 1461),\n",
       "  (1462, 1470),\n",
       "  (1471, 1475),\n",
       "  (1476, 1489),\n",
       "  (1490, 1497),\n",
       "  (1497, 1498),\n",
       "  (1499, 1506),\n",
       "  (1507, 1516),\n",
       "  (1517, 1527),\n",
       "  (1528, 1539),\n",
       "  (1539, 1540),\n",
       "  (1541, 1552),\n",
       "  (1553, 1554),\n",
       "  (1555, 1564),\n",
       "  (1564, 1565),\n",
       "  (1566, 1571),\n",
       "  (1572, 1576),\n",
       "  (1576, 1577),\n",
       "  (1578, 1581),\n",
       "  (1582, 1591),\n",
       "  (1592, 1598),\n",
       "  (1599, 1600),\n",
       "  (1601, 1608),\n",
       "  (1609, 1610),\n",
       "  (1611, 1624),\n",
       "  (1625, 1632),\n",
       "  (1633, 1634),\n",
       "  (1635, 1643),\n",
       "  (1644, 1656),\n",
       "  (1657, 1664),\n",
       "  (1665, 1670),\n",
       "  (1670, 1671),\n",
       "  (1672, 1678),\n",
       "  (1679, 1686),\n",
       "  (1687, 1689),\n",
       "  (1690, 1697),\n",
       "  (1698, 1702),\n",
       "  (1703, 1707),\n",
       "  (1708, 1711),\n",
       "  (1712, 1720),\n",
       "  (1721, 1729),\n",
       "  (1730, 1732),\n",
       "  (1732, 1733),\n",
       "  (1733, 1736),\n",
       "  (1737, 1747),\n",
       "  (1748, 1750),\n",
       "  (1750, 1751),\n",
       "  (1752, 1753),\n",
       "  (1754, 1758),\n",
       "  (1759, 1771),\n",
       "  (1772, 1779),\n",
       "  (1780, 1781),\n",
       "  (1782, 1787),\n",
       "  (1787, 1788)])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags.squeeze().size(), spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = convert_to_submit(tags.squeeze(), spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('STATE_OR_PROVINCE', 'Бостон'), ('ORGANIZATION', 'взорвали'), ('ORGANIZATION', 'Тамерлан'), ('PERCENT', 'и'), ('ORGANIZATION', 'и'), ('TIME', 'Джохар'), ('FAMILY', 'из'), ('ORGANIZATION', '19'), ('COUNTRY', 'пригороде'), ('MONEY', '-'), ('PENALTY', 'марафоне')]\n"
     ]
    }
   ],
   "source": [
    "print([(x[0], text[x[1]:x[2]]) for x in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>senences</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Владелец «Бирмингема» получил шесть лет тюрьмы...</td>\n",
       "      <td>584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Акция протеста на Майдане Независимости объявл...</td>\n",
       "      <td>585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Фольксваген может перейти под контроль Порше \\...</td>\n",
       "      <td>586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>В Москве покажут фильмы Чарли Чаплина с живой ...</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Чулпан Хаматова сыграет главную роль в фильме ...</td>\n",
       "      <td>588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>ОБСЕ назвала референдум о статусе Крыма незако...</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Египетского студента могут выслать из страны з...</td>\n",
       "      <td>645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Геннадий Онищенко отправлен в отставку\\nГеннад...</td>\n",
       "      <td>646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Племянник Алишера Усманова разбился в ДТП\\nВид...</td>\n",
       "      <td>647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Владимир Булавин назначен на новую должность —...</td>\n",
       "      <td>648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             senences   id\n",
       "0   Владелец «Бирмингема» получил шесть лет тюрьмы...  584\n",
       "1   Акция протеста на Майдане Независимости объявл...  585\n",
       "2   Фольксваген может перейти под контроль Порше \\...  586\n",
       "3   В Москве покажут фильмы Чарли Чаплина с живой ...  587\n",
       "4   Чулпан Хаматова сыграет главную роль в фильме ...  588\n",
       "..                                                ...  ...\n",
       "60  ОБСЕ назвала референдум о статусе Крыма незако...  644\n",
       "61  Египетского студента могут выслать из страны з...  645\n",
       "62  Геннадий Онищенко отправлен в отставку\\nГеннад...  646\n",
       "63  Племянник Алишера Усманова разбился в ДТП\\nВид...  647\n",
       "64  Владимир Булавин назначен на новую должность —...  648\n",
       "\n",
       "[65 rows x 2 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = pd.read_json('public_dat/test.jsonl', lines=True)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 584, 'ners': []}, {'id': 585, 'ners': []}, {'id': 586, 'ners': []}, {'id': 587, 'ners': []}, {'id': 588, 'ners': [(1020, 1026, 'COUNTRY')]}, {'id': 589, 'ners': [(462, 467, 'COUNTRY'), (1359, 1372, 'PERCENT')]}, {'id': 590, 'ners': []}, {'id': 591, 'ners': []}, {'id': 592, 'ners': []}, {'id': 593, 'ners': [(580, 582, 'COUNTRY')]}, {'id': 594, 'ners': [(596, 604, 'PERCENT')]}, {'id': 595, 'ners': []}, {'id': 596, 'ners': []}, {'id': 597, 'ners': [(2707, 2713, 'COUNTRY')]}, {'id': 598, 'ners': []}, {'id': 599, 'ners': []}, {'id': 600, 'ners': []}, {'id': 601, 'ners': []}, {'id': 602, 'ners': [(1192, 1193, 'PERCENT')]}, {'id': 603, 'ners': []}, {'id': 604, 'ners': []}, {'id': 605, 'ners': [(93, 97, 'ORGANIZATION'), (190, 196, 'ORGANIZATION')]}, {'id': 606, 'ners': [(495, 498, 'PENALTY')]}, {'id': 607, 'ners': []}, {'id': 608, 'ners': []}, {'id': 609, 'ners': []}, {'id': 610, 'ners': [(836, 837, 'PERCENT')]}, {'id': 611, 'ners': []}, {'id': 612, 'ners': []}, {'id': 613, 'ners': []}, {'id': 614, 'ners': []}, {'id': 615, 'ners': []}, {'id': 616, 'ners': []}, {'id': 617, 'ners': [(228, 230, 'DATE')]}, {'id': 618, 'ners': [(1363, 1366, 'ORGANIZATION')]}, {'id': 619, 'ners': []}, {'id': 620, 'ners': [(642, 647, 'COUNTRY')]}, {'id': 621, 'ners': [(114, 115, 'COUNTRY'), (564, 565, 'COUNTRY')]}, {'id': 622, 'ners': []}, {'id': 623, 'ners': []}, {'id': 624, 'ners': []}, {'id': 625, 'ners': [(882, 883, 'COUNTRY')]}, {'id': 626, 'ners': []}, {'id': 627, 'ners': [(398, 399, 'PENALTY'), (968, 971, 'LAW'), (1144, 1154, 'COUNTRY')]}, {'id': 628, 'ners': []}, {'id': 629, 'ners': []}, {'id': 630, 'ners': []}, {'id': 631, 'ners': []}, {'id': 632, 'ners': [(213, 222, 'COUNTRY')]}, {'id': 633, 'ners': []}, {'id': 634, 'ners': [(222, 233, 'PERCENT')]}, {'id': 635, 'ners': []}, {'id': 636, 'ners': []}, {'id': 637, 'ners': []}, {'id': 638, 'ners': [(1034, 1036, 'PENALTY')]}, {'id': 639, 'ners': []}, {'id': 640, 'ners': []}, {'id': 641, 'ners': []}, {'id': 642, 'ners': []}, {'id': 643, 'ners': []}, {'id': 644, 'ners': []}, {'id': 645, 'ners': []}, {'id': 646, 'ners': []}, {'id': 647, 'ners': []}, {'id': 648, 'ners': []}]\n"
     ]
    }
   ],
   "source": [
    "# Process the test dataset and convert predictions to submission format\n",
    "ans = []\n",
    "for id, text in zip(test_dataset.id, test_dataset.senences):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    spans = [*tokenizer.span_tokenize(text)]\n",
    "    bert_tokens = list(map(bert_tokenizer.convert_tokens_to_ids, tokens))\n",
    "    if len(bert_tokens)>512:\n",
    "        bert_tokens=bert_tokens[:512]\n",
    "    outputs = model(input_ids=torch.tensor(bert_tokens, dtype=torch.long).unsqueeze(0))[1].predictions\n",
    "    tags = (torch.nn.functional.sigmoid(outputs) > 0.69).to(torch.long)\n",
    "    # print(tags)\n",
    "    # print(type(spans))\n",
    "    preds = convert_to_submit(tags.squeeze(), spans)\n",
    "    ans.append({'id':id, 'ners': preds})\n",
    "\n",
    "# Print the submission format predictions\n",
    "print(ans)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Write the submission format predictions to a JSONL file\n",
    "with open('test.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in ans:\n",
    "        # Convert the item to a JSON string and write it to the file\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(ans[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "theses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
